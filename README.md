Modern ML techniques work by introducing many degrees of freedom in a model and then using optimization technniques to tweek the parameters. We want to leverage the power of ML here and see what can be learned about groups. We can start by embedding groups as cayley graphs. A very simple first question is a graph classification problem: given any finite graph G = (V,E), can we tell whether it is the cayley graph of a group? There are many other problems as well: given a finite cayley graph, can we decide whether the group is abelian? Can we find other group invariants: such as space for which it is the fundamental group?

The initial approach is to do unsupervised learning: we will input a large class of cayley graphs and code an algorithm to find structural similarities between these graphs. For instance, Cay(G,S) is an |S| - regular graph i.e. the degree of every vertex is |S|. Thus, the program should learn embeddings of graphs which do not store vertex degree information more than once. In fact, every cayley graph Cay(G,S) corresponds to uniquely to a presentation <S | R> with G being isomorphic to F(S)/<<R>>, where <<R>> denotes the normal closure of R. This indicates that a cayley graph can be represented using less data, ideally a set of symbols S and a set of words on those symbols R. An obvious first choice is to use a feed-forward-autoencoder, compressing the information of the graph.

We begin by producing cayley graphs of various groups. A cayley graph is fully determined by the pair (G,S) where S is a generating set of the group G. Using this definition, we can enumerate all possible cayley finite graphs $C$ by using that famous result in group theory that every finite group is a subgroup of a permutation group. Thus, any cayley graph is of the form $(gen(S), S)$ where S is a finite collection of permutations. 

